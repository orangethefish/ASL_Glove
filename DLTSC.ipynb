{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 284,
      "metadata": {
        "id": "tLj7-e5Q0x3Y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "import seaborn as sns\n",
        "from torch.utils.data import random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 285,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEFsg-ou04cS",
        "outputId": "d7e7c6e1-1347-48f1-d76f-a24bddc198a5"
      },
      "outputs": [],
      "source": [
        "# !gdown --fuzzy https://drive.google.com/file/d/1F9uinZY-eG4x9dNsUOOtAAZMYq6p945U/view?usp=drive_link\n",
        "# !unzip -qq \"ASL-Sensor-Dataglove-Dataset.zip\" -d glove_data\n",
        "# !echo \"Unzip successfully\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 286,
      "metadata": {
        "id": "3dUqCrhchKBE"
      },
      "outputs": [],
      "source": [
        "class MCDCNN(nn.Module):\n",
        "    def __init__(self, input_channels, num_classes, sequence_length):\n",
        "        super(MCDCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(input_channels, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(input_channels, 64, kernel_size=5, padding=2)\n",
        "        self.conv3 = nn.Conv1d(input_channels, 64, kernel_size=7, padding=3)\n",
        "        self.pool = nn.MaxPool1d(2)\n",
        "        self.fc1 = nn.Linear(64 * 3 * (sequence_length // 2), 100)\n",
        "        self.fc2 = nn.Linear(100, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)  # Change shape from [batch_size, sequence_length, input_channels] to [batch_size, input_channels, sequence_length]\n",
        "        x1 = self.relu(self.pool(self.conv1(x)))\n",
        "        x2 = self.relu(self.pool(self.conv2(x)))\n",
        "        x3 = self.relu(self.pool(self.conv3(x)))\n",
        "        x = torch.cat((x1, x2, x3), dim=1)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 287,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "    def __init__(self, in_channels, out_channels, i_downsample=None, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
        "        self.batch_norm1 = nn.BatchNorm2d(out_channels)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.batch_norm2 = nn.BatchNorm2d(out_channels)\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(out_channels, out_channels*self.expansion, kernel_size=1, stride=1, padding=0)\n",
        "        self.batch_norm3 = nn.BatchNorm2d(out_channels*self.expansion)\n",
        "        \n",
        "        self.i_downsample = i_downsample\n",
        "        self.stride = stride\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        identity = x.clone()\n",
        "        \n",
        "        x = self.relu(self.batch_norm1(self.conv1(x)))\n",
        "        \n",
        "        x = self.relu(self.batch_norm2(self.conv2(x)))\n",
        "        \n",
        "        x = self.conv3(x)\n",
        "        x = self.batch_norm3(x)\n",
        "        \n",
        "        #downsample if needed\n",
        "        if self.i_downsample is not None:\n",
        "            identity = self.i_downsample(identity)\n",
        "        #add identity\n",
        "        x+=identity\n",
        "        x=self.relu(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, in_channels, out_channels, i_downsample=None, stride=1):\n",
        "        super(Block, self).__init__()\n",
        "       \n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride, bias=False)\n",
        "        self.batch_norm1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=stride, bias=False)\n",
        "        self.batch_norm2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.i_downsample = i_downsample\n",
        "        self.stride = stride\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "      identity = x.clone()\n",
        "\n",
        "      x = self.relu(self.batch_norm2(self.conv1(x)))\n",
        "      x = self.batch_norm2(self.conv2(x))\n",
        "\n",
        "      if self.i_downsample is not None:\n",
        "          identity = self.i_downsample(identity)\n",
        "      print(x.shape)\n",
        "      print(identity.shape)\n",
        "      x += identity\n",
        "      x = self.relu(x)\n",
        "      return x\n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, ResBlock, layer_list, num_classes, num_channels=3):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(num_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.batch_norm1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size = 3, stride=2, padding=1)\n",
        "        \n",
        "        self.layer1 = self._make_layer(ResBlock, layer_list[0], planes=64)\n",
        "        self.layer2 = self._make_layer(ResBlock, layer_list[1], planes=128, stride=2)\n",
        "        self.layer3 = self._make_layer(ResBlock, layer_list[2], planes=256, stride=2)\n",
        "        self.layer4 = self._make_layer(ResBlock, layer_list[3], planes=512, stride=2)\n",
        "        \n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.fc = nn.Linear(512*ResBlock.expansion, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.batch_norm1(self.conv1(x)))\n",
        "        x = self.max_pool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        \n",
        "        x = self.avgpool(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fc(x)\n",
        "        \n",
        "        return x\n",
        "        \n",
        "    def _make_layer(self, ResBlock, blocks, planes, stride=1):\n",
        "        ii_downsample = None\n",
        "        layers = []\n",
        "        \n",
        "        if stride != 1 or self.in_channels != planes*ResBlock.expansion:\n",
        "            ii_downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.in_channels, planes*ResBlock.expansion, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm2d(planes*ResBlock.expansion)\n",
        "            )\n",
        "            \n",
        "        layers.append(ResBlock(self.in_channels, planes, i_downsample=ii_downsample, stride=stride))\n",
        "        self.in_channels = planes*ResBlock.expansion\n",
        "        \n",
        "        for i in range(blocks-1):\n",
        "            layers.append(ResBlock(self.in_channels, planes))\n",
        "            \n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "        \n",
        "        \n",
        "def ResNet50(num_classes, channels=1):\n",
        "    return ResNet(Bottleneck, [3,4,6,3], num_classes, channels)\n",
        "    \n",
        "def ResNet101(num_classes, channels=3):\n",
        "    return ResNet(Bottleneck, [3,4,23,3], num_classes, channels)\n",
        "\n",
        "def ResNet152(num_classes, channels=3):\n",
        "    return ResNet(Bottleneck, [3,8,36,3], num_classes, channels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 288,
      "metadata": {
        "id": "YiyMUYA7jtP7"
      },
      "outputs": [],
      "source": [
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, root_dir, feature_names=[], channel_size=1):\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        self.label_map = {}\n",
        "        self.feature_names = feature_names\n",
        "        self.channel_size = channel_size\n",
        "        self.load_data(root_dir)\n",
        "\n",
        "    def load_data(self, root_dir):\n",
        "        label_idx = 0\n",
        "        for individual_dir in sorted(os.listdir(root_dir)):\n",
        "            individual_path = os.path.join(root_dir, individual_dir)\n",
        "            for class_dir in sorted(os.listdir(individual_path)):\n",
        "                class_path = os.path.join(individual_path, class_dir)\n",
        "                if os.path.isdir(class_path):\n",
        "                    for file in glob.glob(os.path.join(class_path, \"*.csv\")):\n",
        "                        df = pd.read_csv(file, usecols=self.feature_names)\n",
        "                        data = df.values.astype(np.float32)\n",
        "                        data = data[np.newaxis, :]  \n",
        "                        self.data.append(data)\n",
        "\n",
        "                        \n",
        "                        class_name = os.path.splitext(os.path.basename(file))[0]\n",
        "                        if class_name not in self.label_map:\n",
        "                            self.label_map[class_name] = label_idx\n",
        "                            label_idx += 1\n",
        "                        self.labels.append(self.label_map[class_name])\n",
        "\n",
        "        self.data = np.array(self.data)\n",
        "        self.labels = np.array(self.labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.data[idx]), torch.tensor(self.labels[idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 289,
      "metadata": {
        "id": "wQ2SCOeimP7s"
      },
      "outputs": [],
      "source": [
        "root_dir = 'glove_data'\n",
        "feature_names = [\n",
        "    \"flex_1\", \"flex_2\", \"flex_3\", \"flex_4\", \"flex_5\",\n",
        "    \"GYRx\", \"GYRy\", \"GYRz\",\n",
        "    \"ACCx\", \"ACCy\", \"ACCz\"\n",
        "]\n",
        "\n",
        "\n",
        "dataset = TimeSeriesDataset(root_dir, feature_names)\n",
        "\n",
        "dataset_size = len(dataset)\n",
        "train_size = int(0.7 * dataset_size)  # 80% for training\n",
        "val_size = dataset_size - train_size  # 20% for validation\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders for both sets\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 290,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0hBaApUnzTI",
        "outputId": "83eee43b-ead9-4ff0-d230-fea19acdc49c"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "input_channels = len(feature_names)  # Adjust based on your feature names\n",
        "num_classes = len(dataset.label_map)  # Number of unique classes\n",
        "learning_rate = 0.002\n",
        "num_epochs = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 291,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "fV8mVXePqfeO",
        "outputId": "8c722dfb-86f2-4881-8bb8-430f1e0d3264"
      },
      "outputs": [],
      "source": [
        "# # Initialize model, loss function and optimizer\n",
        "# model = MCDCNN(input_channels=input_channels, num_classes=num_classes, sequence_length=sequence_length).to('cuda')\n",
        "\n",
        "\n",
        "# # Training loop\n",
        "# for epoch in range(num_epochs):\n",
        "#     model.train()  # Set the model to training mode\n",
        "#     running_loss = 0.0\n",
        "#     for inputs, labels in train_dataloader:\n",
        "#         # Verify the input shape before passing it to the model\n",
        "#         inputs, labels = inputs, labels\n",
        "#         inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "\n",
        "#         # Forward pass\n",
        "#         outputs = model(inputs)\n",
        "#         loss = criterion(outputs, labels)\n",
        "\n",
        "#         # Backward pass and optimization\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         running_loss += loss.item()\n",
        "    \n",
        "#     # Calculate and print the average training loss for this epoch\n",
        "#     avg_train_loss = running_loss / len(train_dataloader)\n",
        "#     print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}')\n",
        "\n",
        "#     # Validation phase\n",
        "#     model.eval()  # Set the model to evaluation mode\n",
        "#     val_loss = 0.0\n",
        "#     with torch.no_grad():  # Disable gradient computation\n",
        "#         for val_inputs, val_labels in val_dataloader:\n",
        "#             val_inputs, val_labels = val_inputs, val_labels\n",
        "\n",
        "#             val_inputs, val_labels = val_inputs.to('cuda'), val_labels.to('cuda')\n",
        "\n",
        "#             # Forward pass\n",
        "#             val_outputs = model(val_inputs)\n",
        "#             val_loss += criterion(val_outputs, val_labels).item()\n",
        "    \n",
        "#     # Calculate and print the average validation loss for this epoch\n",
        "#     avg_val_loss = val_loss / len(val_dataloader)\n",
        "#     print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "# print(\"Training complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 292,
      "metadata": {},
      "outputs": [],
      "source": [
        "net = ResNet50(num_classes=num_classes).to('cuda')\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 293,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Train Loss: 4.5607\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/orange/miniconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Validation Loss: 5.1575\n",
            "Epoch [2/20], Train Loss: 3.2060\n",
            "Epoch [2/20], Validation Loss: 32.2451\n",
            "Epoch [3/20], Train Loss: 3.0317\n",
            "Epoch [3/20], Validation Loss: 3663.5059\n",
            "Epoch [4/20], Train Loss: 2.7715\n",
            "Epoch [4/20], Validation Loss: 36.1914\n",
            "Epoch [5/20], Train Loss: 2.6027\n",
            "Epoch [5/20], Validation Loss: 2.6083\n",
            "Epoch [6/20], Train Loss: 2.6595\n",
            "Epoch [6/20], Validation Loss: 39.5837\n",
            "Epoch [7/20], Train Loss: 2.5130\n",
            "Epoch [7/20], Validation Loss: 2.2252\n",
            "Epoch [8/20], Train Loss: 2.1933\n",
            "Epoch [8/20], Validation Loss: 6.5273\n",
            "Epoch [9/20], Train Loss: 2.1118\n",
            "Epoch [9/20], Validation Loss: 2.6939\n",
            "Epoch [10/20], Train Loss: 1.9335\n",
            "Epoch [10/20], Validation Loss: 2.0803\n",
            "Epoch [11/20], Train Loss: 1.9528\n",
            "Epoch [11/20], Validation Loss: 1.7762\n",
            "Epoch [12/20], Train Loss: 1.7648\n",
            "Epoch [12/20], Validation Loss: 1.7625\n",
            "Epoch [13/20], Train Loss: 1.6416\n",
            "Epoch [13/20], Validation Loss: 2.8773\n",
            "Epoch [14/20], Train Loss: 1.7387\n",
            "Epoch [14/20], Validation Loss: 24.2057\n",
            "Epoch [15/20], Train Loss: 2.0956\n",
            "Epoch [15/20], Validation Loss: 9.1468\n",
            "Epoch [16/20], Train Loss: 2.2176\n",
            "Epoch [16/20], Validation Loss: 46.4908\n",
            "Epoch [17/20], Train Loss: 2.2868\n",
            "Epoch [17/20], Validation Loss: 1.9796\n",
            "Epoch [18/20], Train Loss: 2.4171\n",
            "Epoch [18/20], Validation Loss: 280.1528\n",
            "Epoch [19/20], Train Loss: 2.1342\n",
            "Epoch [19/20], Validation Loss: 33.7457\n",
            "Epoch [20/20], Train Loss: 1.9755\n",
            "Epoch [20/20], Validation Loss: 3.1696\n",
            "Training complete\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    net.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_dataloader:\n",
        "        # Verify the input shape before passing it to the model\n",
        "        inputs, labels = inputs, labels\n",
        "        inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    \n",
        "    # Calculate and print the average training loss for this epoch\n",
        "    avg_train_loss = running_loss / len(train_dataloader)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}')\n",
        "\n",
        "    # Validation phase\n",
        "    net.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "        for val_inputs, val_labels in val_dataloader:\n",
        "            val_inputs, val_labels = val_inputs, val_labels\n",
        "            val_inputs, val_labels = val_inputs.to('cuda'), val_labels.to('cuda')\n",
        "\n",
        "            # Forward pass\n",
        "            val_outputs = net(val_inputs)\n",
        "            val_loss += criterion(val_outputs, val_labels).item()\n",
        "    \n",
        "    # Calculate and print the average validation loss for this epoch\n",
        "    avg_val_loss = val_loss / len(val_dataloader)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "print(\"Training complete\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
