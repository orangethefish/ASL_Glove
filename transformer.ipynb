{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 16:55:50.559946: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-24 16:55:50.627868: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-24 16:55:50.644124: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-24 16:55:50.764035: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-24 16:55:51.896666: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Concatenate\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset:\n",
    "    def __init__(self, root_dir, feature_names=[]):\n",
    "        self.data = self.load_data(root_dir, feature_names)\n",
    "\n",
    "    def load_data(self, root_dir, feature_names):\n",
    "        data = []\n",
    "\n",
    "        for individual_dir in sorted(os.listdir(root_dir)):\n",
    "            individual_path = os.path.join(root_dir, individual_dir)\n",
    "            for class_dir in sorted(os.listdir(individual_path)):\n",
    "                class_path = os.path.join(individual_path, class_dir)\n",
    "                if os.path.isdir(class_path):\n",
    "                    for file in glob.glob(os.path.join(class_path, \"*.csv\")):\n",
    "                        df = pd.read_csv(file, usecols=feature_names)\n",
    "                        class_name = os.path.splitext(os.path.basename(file))[0]\n",
    "                        df[\"class\"] = class_name\n",
    "                        data.append(df)\n",
    "\n",
    "\n",
    "        # Concatenate all data frames into a single data frame\n",
    "        data = pd.concat(data, ignore_index=True)\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"glove_data\"\n",
    "feature_names = [\n",
    "    \"flex_1\", \"flex_2\", \"flex_3\", \"flex_4\", \"flex_5\",\n",
    "    \"GYRx\", \"GYRy\", \"GYRz\",\n",
    "    \"ACCx\", \"ACCy\", \"ACCz\"\n",
    "]\n",
    "\n",
    "dataset = TimeSeriesDataset(root_dir, feature_names).data\n",
    "# dataset = dataset.sort_values(by=[\"class\"])\n",
    "\n",
    "# filter_classes = [\"deaf\", \"fine\", \"good\", \"goodbye\", \"hello\"]\n",
    "# dataset = dataset[dataset[\"class\"].isin(filter_classes)]\n",
    "\n",
    "x_train, y_train = dataset.iloc[:, :-1].values, dataset.iloc[:, -1].values\n",
    "\n",
    "# x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
    "\n",
    "\n",
    "timesteps = 150\n",
    "n_features = 11\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "idx = np.random.permutation(len(x_train))\n",
    "x_train = x_train[idx]\n",
    "y_train = y_train[idx]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Attention and Normalization\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(inputs, inputs)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(res)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    return x + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_last\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    return Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_shape = x_train.shape[1:]\n",
    "\n",
    "\n",
    "# model = build_model(\n",
    "#     input_shape,\n",
    "#     head_size=256,\n",
    "#     num_heads=4,\n",
    "#     ff_dim=4,\n",
    "#     num_transformer_blocks=4,\n",
    "#     mlp_units=[128],\n",
    "#     mlp_dropout=0.4,\n",
    "#     dropout=0.25,\n",
    "# )\n",
    "\n",
    "# model.compile(\n",
    "#     loss=\"sparse_categorical_crossentropy\",\n",
    "#     optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "#     metrics=[\"sparse_categorical_accuracy\"],\n",
    "# )\n",
    "# model.summary()\n",
    "\n",
    "# callbacks = [tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
    "\n",
    "# model.fit(\n",
    "#     x_train,\n",
    "#     y_train,\n",
    "#     epochs=1,\n",
    "#     batch_size=64,\n",
    "#     validation_split=0.2,\n",
    "#     callbacks=callbacks,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, QuantileTransformer, PowerTransformer, Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "dict_classifiers = {\n",
    "  \"Logistic Regression\": LogisticRegression(max_iter=1000,solver='saga'),\n",
    "  \"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "  \"RBF SVM\": SVC(C=10000,gamma=0.1),\n",
    "  \"Linear SVM\": SVC(kernel='linear'),\n",
    "  \"Gradient Boosting Classifier\": GradientBoostingClassifier(n_estimators=100),\n",
    "  \"Decision Tree\": tree.DecisionTreeClassifier(),\n",
    "  \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "  \"Neural Net\": MLPClassifier(alpha = 1),\n",
    "  \"Naive Bayes\": GaussianNB(),\n",
    "  \"AdaBoost\": AdaBoostClassifier(),\n",
    "  \"QDA\": QuadraticDiscriminantAnalysis(),\n",
    "  \"Gaussian Process\": GaussianProcessClassifier()\n",
    "}\n",
    "\n",
    "dict_normalizers = {\n",
    "  'No Normalizer': None,\n",
    "  'StandardScaler': StandardScaler(),\n",
    "  'MinMaxScaler': MinMaxScaler(),\n",
    "  'MaxAbsScaler': MaxAbsScaler(),\n",
    "  'RobustScaler': RobustScaler(),\n",
    "  'QuantileTransformer-Normal': QuantileTransformer(output_distribution='normal'),\n",
    "  'QuantileTransformer-Uniform': QuantileTransformer(output_distribution='uniform'),\n",
    "  'PowerTransformer-Yeo-Johnson': PowerTransformer(method='yeo-johnson'),\n",
    "  'Normalizer': Normalizer()\n",
    "}\n",
    "\n",
    "\n",
    "# def batch_classify(X_train, Y_train, X_test, Y_test, verbose = True):\n",
    "#   \"\"\"\n",
    "#   This method, takes as input the X, Y matrices of the Train and Test set.\n",
    "#   And fits them on all of the Classifiers specified in the dict_classifier.\n",
    "#   The trained models, and accuracies are saved in a dictionary. The reason to use a dictionary\n",
    "#   is because it is very easy to save the whole dictionary with the pickle module.\n",
    "\n",
    "#   Usually, the SVM, Random Forest and Gradient Boosting Classifier take quiet some time to train.\n",
    "#   So it is best to train them on a smaller dataset first and\n",
    "#   decide whether you want to comment them out or not based on the test accuracy score.\n",
    "#   \"\"\"\n",
    "\n",
    "#   dict_models = []\n",
    "#   for classifier_name, classifier in list(dict_classifiers.items()):\n",
    "#     for normalizer_name, normalizer in list(dict_normalizers.items()):\n",
    "#       t_start = time.time()\n",
    "#       pipe = Pipeline([('nomalizer', normalizer), ('clf', classifier)])\n",
    "#       pipe.fit(X_train, Y_train)\n",
    "#       t_end = time.time()\n",
    "\n",
    "#       t_diff = t_end - t_start\n",
    "#       train_score = pipe.score(X_train, Y_train)\n",
    "#       test_score = pipe.score(X_test, Y_test)\n",
    "\n",
    "#       dict_models.append([classifier_name, normalizer_name, train_score,\n",
    "#                           test_score, t_diff])\n",
    "\n",
    "#       if verbose:\n",
    "#         print(\"trained {c} in {f:.2f} s\".format(c=classifier_name + '__' + normalizer_name, f=t_diff))\n",
    "#   return dict_models\n",
    "\n",
    "\n",
    "# def display_dict_models(dict_models, sort_by=\"Test_Score\"):\n",
    "#   df = pd.DataFrame(dict_models, columns=[\"Classifier\", \"Normalizer\", \n",
    "#                                           \"Train_Score\", \"Test_Score\", \n",
    "#                                           \"Train_Time\"])\n",
    "  \n",
    "#   df = df.sort_values(by=sort_by, ascending=False)\n",
    "#   display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m classifier \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandom Forest\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m#@param [\"Logistic Regression\", \"Nearest Neighbors\", \"RBF SVM\", \"Linear SVM\", \"Gradient Boosting Classifier\", \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"Naive Bayes\", \"AdaBoost\", \"QDA\", \"Gaussian Process\"]\u001b[39;00m\n\u001b[1;32m      2\u001b[0m clf \u001b[38;5;241m=\u001b[39m dict_classifiers[classifier]\n\u001b[0;32m----> 5\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(x_test)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m%0.2f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m(accuracy_score(y_test,y_pred)))\n",
      "File \u001b[0;32m~/miniconda3/envs/ASL_glove/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ASL_glove/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    481\u001b[0m ]\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/miniconda3/envs/ASL_glove/lib/python3.12/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ASL_glove/lib/python3.12/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/miniconda3/envs/ASL_glove/lib/python3.12/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/miniconda3/envs/ASL_glove/lib/python3.12/site-packages/sklearn/utils/parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ASL_glove/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:192\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    190\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[0;32m--> 192\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[1;32m    201\u001b[0m         X,\n\u001b[1;32m    202\u001b[0m         y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[1;32m    206\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/ASL_glove/lib/python3.12/site-packages/sklearn/tree/_classes.py:305\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    303\u001b[0m y_encoded \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(y\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_):\n\u001b[0;32m--> 305\u001b[0m     classes_k, y_encoded[:, k] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mappend(classes_k)\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_\u001b[38;5;241m.\u001b[39mappend(classes_k\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/ASL_glove/lib/python3.12/site-packages/numpy/lib/arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[1;32m    272\u001b[0m ar \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(ar)\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43m_unique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mequal_nan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ASL_glove/lib/python3.12/site-packages/numpy/lib/arraysetops.py:333\u001b[0m, in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[1;32m    330\u001b[0m optional_indices \u001b[38;5;241m=\u001b[39m return_index \u001b[38;5;129;01mor\u001b[39;00m return_inverse\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optional_indices:\n\u001b[0;32m--> 333\u001b[0m     perm \u001b[38;5;241m=\u001b[39m \u001b[43mar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margsort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmergesort\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquicksort\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar[perm]\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classifier = \"Random Forest\" #@param [\"Logistic Regression\", \"Nearest Neighbors\", \"RBF SVM\", \"Linear SVM\", \"Gradient Boosting Classifier\", \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"Naive Bayes\", \"AdaBoost\", \"QDA\", \"Gaussian Process\"]\n",
    "clf = dict_classifiers[classifier]\n",
    "\n",
    "\n",
    "clf.fit(x_train,y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "print(\"Test Accuracy: %0.2f\" %(accuracy_score(y_test,y_pred)))\n",
    "if len(set(y_test)) > len(set(y_pred)):\n",
    "  print(\"The model did not predict the following classes at all...\")\n",
    "  print(label_encoder.inverse_transform(list(set(y_test.astype(int)) - set(y_pred.astype(int)))))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7506\n",
      "           1       1.00      1.00      1.00      7651\n",
      "           2       0.98      0.98      0.98      7436\n",
      "           3       1.00      0.99      1.00      7654\n",
      "           4       1.00      1.00      1.00      7458\n",
      "           5       0.99      0.99      0.99      7545\n",
      "           6       0.99      0.99      0.99      7452\n",
      "           7       1.00      1.00      1.00      7552\n",
      "           8       0.99      1.00      0.99      7434\n",
      "           9       1.00      1.00      1.00      7565\n",
      "          10       0.98      0.98      0.98      7405\n",
      "          11       0.99      0.99      0.99      7423\n",
      "          12       1.00      1.00      1.00      7406\n",
      "          13       0.99      0.99      0.99      7494\n",
      "          14       1.00      0.99      1.00      7498\n",
      "          15       1.00      1.00      1.00      7515\n",
      "          16       1.00      1.00      1.00      7620\n",
      "          17       1.00      1.00      1.00      7457\n",
      "          18       1.00      1.00      1.00      7418\n",
      "          19       1.00      1.00      1.00      7572\n",
      "          20       0.99      0.99      0.99      7558\n",
      "          21       1.00      1.00      1.00      7537\n",
      "          22       1.00      1.00      1.00      7481\n",
      "          23       1.00      1.00      1.00      7544\n",
      "          24       1.00      1.00      1.00      7536\n",
      "          25       0.99      1.00      0.99      7591\n",
      "          26       1.00      1.00      1.00      7426\n",
      "          27       1.00      1.00      1.00      7478\n",
      "          28       1.00      1.00      1.00      7554\n",
      "          29       1.00      1.00      1.00      7589\n",
      "          30       1.00      1.00      1.00      7379\n",
      "          31       0.98      0.98      0.98      7421\n",
      "          32       1.00      1.00      1.00      7681\n",
      "          33       1.00      1.00      1.00      7453\n",
      "          34       1.00      1.00      1.00      7426\n",
      "          35       1.00      1.00      1.00      7609\n",
      "          36       1.00      1.00      1.00      7370\n",
      "          37       0.99      0.99      0.99      7340\n",
      "          38       0.99      0.99      0.99      7419\n",
      "          39       1.00      1.00      1.00      7547\n",
      "\n",
      "    accuracy                           1.00    300000\n",
      "   macro avg       1.00      1.00      1.00    300000\n",
      "weighted avg       1.00      1.00      1.00    300000\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "too many positional arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_facecolor(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxkcd:white\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Plot Confusion Matrix\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabel_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# plot_confusion_matrix(clf, X_test, y_test, ax=ax, display_labels=list(label_encoder.classes_),\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#                             cmap=plt.cm.Blues,\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#                             normalize=None)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m plt\u001b[38;5;241m.\u001b[39mxticks(rotation\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m90\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ASL_glove/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:191\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_sig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m params\u001b[38;5;241m.\u001b[39mapply_defaults()\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# ignore self/cls and positional/keyword markers\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ASL_glove/lib/python3.12/inspect.py:3267\u001b[0m, in \u001b[0;36mSignature.bind\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   3263\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[1;32m   3264\u001b[0m \u001b[38;5;124;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[1;32m   3265\u001b[0m \u001b[38;5;124;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[1;32m   3266\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ASL_glove/lib/python3.12/inspect.py:3191\u001b[0m, in \u001b[0;36mSignature._bind\u001b[0;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[1;32m   3187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3188\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m (_VAR_KEYWORD, _KEYWORD_ONLY):\n\u001b[1;32m   3189\u001b[0m         \u001b[38;5;66;03m# Looks like we have no parameter for this positional\u001b[39;00m\n\u001b[1;32m   3190\u001b[0m         \u001b[38;5;66;03m# argument\u001b[39;00m\n\u001b[0;32m-> 3191\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   3192\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoo many positional arguments\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3194\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m _VAR_POSITIONAL:\n\u001b[1;32m   3195\u001b[0m         \u001b[38;5;66;03m# We have an '*args'-like argument, let's fill it with\u001b[39;00m\n\u001b[1;32m   3196\u001b[0m         \u001b[38;5;66;03m# all positional arguments we have left and move on to\u001b[39;00m\n\u001b[1;32m   3197\u001b[0m         \u001b[38;5;66;03m# the next phase\u001b[39;00m\n\u001b[1;32m   3198\u001b[0m         values \u001b[38;5;241m=\u001b[39m [arg_val]\n",
      "\u001b[0;31mTypeError\u001b[0m: too many positional arguments"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_classification_report = True #@param {type:\"boolean\"}\n",
    "if show_classification_report:\n",
    "  print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "show_confusion_matrix = False #@param {type:\"boolean\"}\n",
    "if show_confusion_matrix:\n",
    "  fig, ax = plt.subplots(figsize=(15, 15))\n",
    "  title = \"Confusion Matrix\"\n",
    "  ax.set_title(title)\n",
    "  ax.set_facecolor('xkcd:white')\n",
    "\n",
    "  # Plot Confusion Matrix\n",
    "  confusion_matrix(clf, x_test, y_test, ax=ax, display_labels=list(label_encoder.classes_))\n",
    "  # plot_confusion_matrix(clf, X_test, y_test, ax=ax, display_labels=list(label_encoder.classes_),\n",
    "  #                             cmap=plt.cm.Blues,\n",
    "  #                             normalize=None)\n",
    "\n",
    "  plt.xticks(rotation=90)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_models = batch_classify(x_train, y_train, x_test, y_test)\n",
    "# display_dict_models(dict_models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ASL_glove",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
